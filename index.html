<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="assets/css/reset.css">
    <link rel="stylesheet" type="text/css" href="assets/css/stylesheet.css">
    <script src="assets/js/simplejs.js"></script>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="assets/js/youtube.js"></script>
    <link rel="icon" href="assets/images/dut.png">

    <title>Phan Binh</title>
  </head>
  
  <body class="body">
    <nav class="navigation block">
        <ul>
            <li>
                <a href="index.html" style="font-size:16px"> Home</a>
            </li>
            <li>
                <a href="index.html#projects" style="font-size:16px"> Projects</a>
            </li>
            <li>
                <a href="blog.html" style="font-size:16px"> Tech Blog</a>
            </li>
        </ul>
    </nav>
    <div class="divider"></div>
    <div class="introduction block" style="display:flex">
        <div class="" style="width:30%;float:left;">
            <img src="assets/images/binh.png" 
            style="width:190px;height:190px;border-radius:50%;display: table-cell; margin:auto;position: relative;
            top: 50%; transform: translateY(-50%);">
        </div>

        <div class="" style="width:70%;float:left">
            <p class="name block" align="center">Phan Binh </p>

            <p class="block">
                As an Undergraduate Student at <a href="https://dut.udn.vn/" target="_blank">The University of Da Nang - University of Science and Technology (DUT)</a>, 
                I have developed and optimized multimodal deep learning and computer vision models, including attention-based architectures, 
                for applications such as product categorization and manure identification. These models were designed to achieve high accuracy and efficiency, 
                effectively handling imbalanced data and performing well in resource-constrained environments, ensuring robust and real-time performance.
            </p>
            
            <p class="block">
                I specialize in real-time change detection, object detection, single-camera tracking, multi-camera tracking, 
                generative AI, and crowd estimation. My work focuses on deploying advanced AI models on embedded platforms 
                and optimizing them for real-world applications.
            </p>
            

            <p class="block">
                <a href="mailto:binhphan77373@gmail.com">Email: binhphan77373@gmail.com</a>
                <!-- <span>&sol;</span> -->
                <br>
                <a href="assets/pdf/CV_Phan_Binh_2025.pdf" target="_blank">CV</a> 
                <span>&sol;</span> 
                <a href="https://www.linkedin.com/in/prog1405/" target="_blank">Linkedin</a>
                <span>&sol;</span> 
                <a href="https://github.com/binhphan77373" target="_blank">Github</a>
            </p>
        </div>
    </div>
    <div class="divider"></div>
    <div class="research block">
        <p class="heading block"> Research</p>
        <p class="block">My research focuses on developing and optimizing multimodal deep learning and computer vision models, 
            including attention-based architectures, for real-world applications such as product categorization, manure identification, 
            and AI-powered assistive technologies. I specialize in creating solutions that achieve high accuracy and efficiency while 
            effectively handling imbalanced data and performing well in resource-constrained environments.
        </p>
        <p class="block">
            My expertise spans various technical frameworks including Python, PyTorch, TensorFlow, ONNX, OpenCV, and ROS2. 
            I have extensive experience in data preparation, model optimization techniques (quantization, pruning), and diverse 
            learning methodologies including transfer learning, supervised, self-supervised, and unsupervised learning. 
            My practical work includes real-time change detection, object detection, image classification, image segmentation, 
            crowd counting, generative AI, and single/multi-camera tracking, with a strong emphasis on deploying these models 
            on embedded devices like NVIDIA Jetson Orin NX, Intel NUC, Odroid N2+, and Arduino microcontrollers.

            <p class="block">
            <strong>If you are interested in Computer Vision, Deep Learning, and Embedded Systems, feel free to <a href="mailto:binhphan77373@gmail.com">contact me</a> for collaboration or mentorship in these areas.</strong>
            </p>

        </p>
    </div>

    <div class="divider"></div>
    <div class="workexp block" id="workexp">
        <p class="heading block">Work Experience</p>
        <div class="research-proj block">
            <div class="research-summary">
                <div style="overflow:hidden; margin-bottom: 5px;">
                    <div style="float:left;">
                        <span class="pub-name" style="color:black;">AI Team, VJ TECHNOLOGIES CO.LTD</span>
                    </div>
                    <div style="float:right;">
                        <span style="font-style:normal;">Da Nang, Viet Nam</span>
                    </div>
                </div>
                <div style="overflow:hidden; margin-bottom: 5px;">
                    <div style="float:left;">
                        <span style="font-style:italic;">Research Engineer | AI, Computer Vision</span>
                    </div>
                    <div style="float:right;">
                        <span style="font-style:normal;">Jul. 2025 – Present</span>
                    </div>
                </div>

                <p class="pub-intro" style="font-size:14px; padding:5px 0px 5px 0px;">
                    <strong>PV Module Layout Optimization & Rooftop Detection System:</strong><br>
                    • Designed a 2D bin-packing optimizer for PV module layouts on single- to four-slope roofs, honoring code setbacks, obstructions, azimuth/tilt, and spacing constraints to maximize energy yield and roof utilization.<br>
                    • Reconstructed roof geometry from plan dimensions (CAD/PDF) using Gemini Vision API to extract building dimensions and roof gable overhang orientation from Japanese architectural drawings, with multi-page PDF processing.<br>
                    • Built a computer-vision pipeline to detect and vectorize rooftop facets from aerial imagery: YOLO11x-seg for roof polygon detection, ridge line detection using Hough Transform, and roof type classification (1-4 slopes) with DBSCAN clustering and RDP simplification.<br>
                    • <strong>Tech:</strong> Python, OpenCV, NumPy, Gemini Vision API, PDF2Image; <strong>Models:</strong> YOLO11x-seg; <strong>Methods:</strong> Dynamic Programming, Knapsack algorithm, CLAHE preprocessing, adaptive thresholding, DBSCAN, RDP simplification, Hough Transform, multi-threaded processing.<br><br>
                    <strong>Automated Document Processing System for Japanese Real Estate Using Vision-Language Models:</strong><br>
                    • Developed an automated document processing system for Japanese real estate using Vision-Language Models (QwenVL) to extract and populate data from PDFs into Excel templates.<br>
                    • Implemented end-to-end pipeline for document digitization: detecting empty cells in templates, extracting structured data from PDF documents, mapping extracted information to predefined schemas, and generating formatted output files.<br>
                    • Built REST API with FastAPI, asynchronous processing, callback support, and error handling.<br>
                    • Integrated LLM for table detection, OCR, and form filling with prompt engineering.<br>
                    • <strong>Results:</strong> automated document processing workflow, reduced processing time from manual to automated, high accuracy with Japanese text and complex tables.<br>
                    • <strong>Technologies:</strong> Python, FastAPI, QwenVL, Vision-Language Models, OCR, PDF Processing, Excel Automation, RESTful APIs, Docker, Pydantic.
                </p>
            </div>
            
        </div>
        <div class="research-proj block">
            <div class="research-summary">
                <div style="overflow:hidden; margin-bottom: 5px;">
                    <div style="float:left;">
                        <span class="pub-name" style="color:black;">Center for Advance Robotics Innovation Technology, Nanyang Technological University</span>
                    </div>
                    <div style="float:right;">
                        <span style="font-style:normal;">Nanyang, Singapore</span>
                    </div>
                </div>
                <div style="overflow:hidden; margin-bottom: 5px;">
                    <div style="float:left;">
                        <span style="font-style:italic;">Research Associate | AI, Computer Vision & Embedded System</span>
                    </div>
                    <div style="float:right;">
                        <span style="font-style:normal;">Mar. 2025 – Apr. 2025</span>
                    </div>
                </div>

                <p class="pub-intro" style="font-size:14px; padding:5px 0px 5px 0px;">
                    <strong>AI-powered smart glasses for the visually impaired:</strong><br>
                    • Integrated object detection, depth estimation, text to speech, speech to text, and vision-language models on embedded platforms like NVIDIA Jetson Orin NX, Intel NUC, and Aria smart glasses to assist visually impaired users. Achieved real-time performance up to 14 FPS on Jetson Orin NX.<br>
                    • Enabled real-time assistance for visually impaired users by guiding them to locate specific objects upon request and navigate safely by detecting and avoiding obstacles along their path.
                </p>
            </div>
            
        </div>
        <div class="research-proj block">
            <div class="research-summary">
                <div style="overflow:hidden; margin-bottom: 5px;">
                    <div style="float:left;">
                        <span class="pub-name" style="color:black;">AI Team, EyeCode company</span>
                    </div>
                    <div style="float:right;">
                        <span style="font-style:normal;">Da Nang, Viet Nam</span>
                    </div>
                </div>
                <div style="overflow:hidden; margin-bottom: 5px;">
                    <div style="float:left;">
                        <span style="font-style:italic;">Research Engineer | AI, Computer Vision</span>
                    </div>
                    <div style="float:right;">
                        <span style="font-style:normal;">May. 2024 – Oct. 2024</span>
                    </div>
                </div>

                <p class="pub-intro" style="font-size:14px; padding:5px 0px 5px 0px;">
                    <strong>YOLOv5s Pruning Optimization:</strong> Pruning YOLOv5s by 10% reduced inference time by around 5%, with a minor mAP drop (from 0.812 to 0.797). At 50% pruning, inference time decreased further, but mAP dropped to 0.654. Performance declined significantly beyond 50% pruning. Tests were conducted on an Odroid N2+.<br><br>
                    <strong>YOLOv5s Transfer Learning:</strong> Enhanced the YOLOv5s model using Transfer Learning with pre-trained VOC weights, improving object detection on a smaller dataset. The mAP increased from 0.695 to 0.812, then further to 0.846, demonstrating significant accuracy gains with limited data. A slight increase in inference time on the Odroid N2+ (up by 0.88%) was deemed negligible compared to the accuracy improvement.<br><br>
                    <strong>Quantization YOLOv5s:</strong> Applied Quantization techniques to compress the YOLOv5s model, utilizing both Post-training Quantization and Quantization Aware Training. Post-training Quantization reduced inference time by 3.57% with a slight mAP drop (from 0.824 to 0.806). Quantization Aware Training further enhanced speed (reduced by 9.45% compared to the original model), maintaining mAP at 0.819 with minimal accuracy loss. These results, tested on the Odroid N2+, achieved faster inference times with high accuracy retention.
                </p>
            </div>
            
        </div>
        <div class="research-proj block">
            <div class="research-summary">
                <div style="overflow:hidden; margin-bottom: 5px;">
                    <div style="float:left;">
                        <span class="pub-name" style="color:black;">AI Team, D-soft jsc</span>
                    </div>
                    <div style="float:right;">
                        <span style="font-style:normal;">Da Nang, Viet Nam</span>
                    </div>
                </div>
                <div style="overflow:hidden; margin-bottom: 5px;">
                    <div style="float:left;">
                        <span style="font-style:italic;">Research Engineer | AI, Computer Vision</span>
                    </div>
                    <div style="float:right;">
                        <span style="font-style:normal;">Jan. 2024 – Apr. 2024</span>
                    </div>
                </div>

                <p class="pub-intro" style="font-size:14px; padding:5px 0px 5px 0px;">
                    <strong>Counting people in the ROI area of many CCTVs:</strong><br>
                    • <strong>Locating and Analyzing Areas of Interest:</strong> Users can seamlessly select and analyze specific areas within the video stream by clicking on optional points that define the corners of the Region of Interest (ROI).<br>
                    • <strong>Dynamic People Detection and Counting:</strong> The system leverages the YOLOv8n model to detect and count individuals within the ROI, delivering real-time updates with approximately 98% accuracy, a processing time of about 40ms per frame, and a performance of around 25 fps on an NVIDIA GTX 1650 (4GB).<br>
                    • <strong>Advanced Analytics:</strong> By integrating cutting-edge technologies such as OpenCV and Streamlit, the platform delivers robust data visualization and real-time analytical capabilities.
                </p>
            </div>
            
        </div>
        <div class="research-proj block">
            <div class="research-summary">
                <div style="overflow:hidden; margin-bottom: 5px;">
                    <div style="float:left;">
                        <span class="pub-name" style="color:black;">IT department at DUT</span>
                    </div>
                    <div style="float:right;">
                        <span style="font-style:normal;">Da Nang City, Viet Nam</span>
                    </div>
                </div>
                <div style="overflow:hidden; margin-bottom: 5px;">
                    <div style="float:left;">
                        <span style="font-style:italic;">Research Assistant | AI, Computer Vision</span>
                    </div>
                    <div style="float:right;">
                        <span style="font-style:normal;">Dec. 2023 – Feb. 2024</span>
                    </div>
                </div>

                <p class="pub-intro" style="font-size:14px; padding:5px 0px 5px 0px;">
                    <strong>Enhanced Attention-based Multimodal Deep Learning for Product Categorization on E-commerce Platform:</strong><br>
                    • <strong>Model Design and Implementation:</strong> Designed and implemented an attention-based multimodal deep learning model to improve the accuracy of product classification on e-commerce platforms. The model integrates image and text data using a robust fusion module with attention mechanisms to classify 16 product categories.<br>
                    • <strong>Performance and Impact:</strong> The proposed model achieved a significant accuracy of 91.18%, outperforming traditional multimodal and unimodal deep learning models (which reached a maximum of 77.21%). This improvement enhances product searchability and the overall customer experience.<br>
                    • <strong>Personal Role:</strong> My role involved building and optimizing the deep learning architecture, focusing on fusing multimodal data for better product categorization. This model contributes to solving the challenge of automatic product classification, with practical implications for e-commerce platform management.
                </p>
            </div>
            
        </div>
        <div class="research-proj block">
            <div class="research-summary">
                <div style="overflow:hidden; margin-bottom: 5px;">
                    <div style="float:left;">
                        <span class="pub-name" style="color:black;">IT department at DUT</span>
                    </div>
                    <div style="float:right;">
                        <span style="font-style:normal;">Da Nang City, Viet Nam</span>
                    </div>
                </div>
                <div style="overflow:hidden; margin-bottom: 5px;">
                    <div style="float:left;">
                        <span style="font-style:italic;">Research Assistant | AI, Signal Processing</span>
                    </div>
                    <div style="float:right;">
                        <span style="font-style:normal;">Sep. 2023 – Nov. 2023</span>
                    </div>
                </div>

                <p class="pub-intro" style="font-size:14px; padding:5px 0px 5px 0px;">
                    <strong>The modified Vision Transformer for imbalanced NIRs data classification:</strong><br>
                    • <strong>Model Development:</strong> Led the design and implementation of NIRsViT, a deep learning model based on the Vision Transformer architecture. This model was specifically tailored for manure classification using near-infrared spectroscopy (NIRS) data to improve identification accuracy across various manure types.<br>
                    • <strong>Innovative Methods and Performance:</strong> Introduced innovative methods such as Focal Loss and Upsampling to handle imbalanced datasets common in agricultural data. This approach significantly enhanced classification performance, achieving an F1-Score of 93.03% and an accuracy of 97.96%, outperforming existing models.<br>
                    • <strong>Responsibilities and Contribution:</strong> My responsibilities encompassed developing the deep learning model, preprocessing data, and optimizing classification algorithms. This project established a new benchmark in NIRS-based manure identification, laying a foundation for future research.
                </p>
            </div>
            
        </div>
    </div>

    <div class="divider"></div>
    <div class="project block" id="projects">
        <p class="heading block">Projects</p>
        <div class="research-proj block">
            <div class="research-summary">
                <div style="overflow:hidden">
                    <div style="max-width: 50%; float:left;">
                        <a href="https://drive.google.com/file/d/1UQzgzMT9CBYTIjxYRiWUfnkSKK68cS4e/view?usp=sharing" class="pub-name" style="display:inline" target="_blank">AI-powered Smart Glasses for Visually Impaired</a>
                    </div>
                    <div style="max-width: 50%; float:right;">
                        NTU, 2025
                    </div>
                </div>

                <p class="pub-intro" style="font-size:14px; padding:5px 0px 5px 0px;">
                    Integrated object detection, depth estimation, text to speech, speech to text, and vision-language models on embedded platforms (NVIDIA Jetson Orin NX, Intel NUC, Aria smart glasses) to assist visually impaired users, achieving real-time performance up to 14 FPS. The system enables real-time assistance by guiding users to locate specific objects and navigate safely by detecting and avoiding obstacles.
                </p>
            </div>
        </div>
        <div class="research-proj block">
            <div class="research-summary">
                <div style="overflow:hidden">
                    <div style="max-width: 50%; float:left;">
                        <span class="pub-name" style="color:black;">PV Module Layout Optimization & Rooftop Detection System</span>
                    </div>
                    <div style="max-width: 50%; float:right;">
                        VJ Technologies, 2025
                    </div>
                </div>

                <p class="pub-intro" style="font-size:14px; padding:5px 0px 5px 0px;">
                    Designed a 2D bin-packing optimizer for PV module layouts on single- to four-slope roofs, honoring code setbacks, obstructions, azimuth/tilt, and spacing constraints to maximize energy yield and roof utilization. Reconstructed roof geometry from plan dimensions (CAD/PDF) using Gemini Vision API to extract building dimensions and roof gable overhang orientation from Japanese architectural drawings, with multi-page PDF processing. Built a computer-vision pipeline to detect and vectorize rooftop facets from aerial imagery: YOLO11x-seg for roof polygon detection, ridge line detection using Hough Transform, and roof type classification (1-4 slopes) with DBSCAN clustering and RDP simplification. Technologies: Python, OpenCV, NumPy, Gemini Vision API, PDF2Image; Models: YOLO11x-seg; Methods: Dynamic Programming, Knapsack algorithm, CLAHE preprocessing, adaptive thresholding, DBSCAN, RDP simplification, Hough Transform, multi-threaded processing.
                </p>
            </div>
        </div>
        <div class="research-proj block">
            <div class="research-summary">
                <div style="overflow:hidden">
                    <div style="max-width: 50%; float:left;">
                        <span class="pub-name" style="color:black;">Automated Document Processing System for Japanese Real Estate Using Vision-Language Models</span>
                    </div>
                    <div style="max-width: 50%; float:right;">
                        VJ Technologies, 2025
                    </div>
                </div>

                <p class="pub-intro" style="font-size:14px; padding:5px 0px 5px 0px;">
                    Developed an automated document processing system for Japanese real estate using Vision-Language Models (QwenVL) to extract and populate data from PDFs into Excel templates. Implemented end-to-end pipeline for document digitization: detecting empty cells in templates, extracting structured data from PDF documents, mapping extracted information to predefined schemas, and generating formatted output files. Built REST API with FastAPI, asynchronous processing, callback support, and error handling. Integrated LLM for table detection, OCR, and form filling with prompt engineering. Results: automated document processing workflow, reduced processing time from manual to automated, high accuracy with Japanese text and complex tables. Technologies: Python, FastAPI, QwenVL, Vision-Language Models, OCR, PDF Processing, Excel Automation, RESTful APIs, Docker, Pydantic.
                </p>
            </div>
        </div>
        <div class="research-proj block">
            <div class="research-summary">
                <div style="overflow:hidden">
                    <div style="max-width: 50%; float:left;">
                        <span class="pub-name" style="color:black;">YOLOv5s Optimization: Pruning, Transfer Learning & Quantization</span>
                    </div>
                    <div style="max-width: 50%; float:right;">
                        EyeCode, 2024
                    </div>
                </div>

                <p class="pub-intro" style="font-size:14px; padding:5px 0px 5px 0px;">
                    Optimized YOLOv5s model through multiple techniques: (1) Pruning by 10% reduced inference time by 5% with minor mAP drop (0.812 to 0.797); (2) Transfer Learning improved mAP from 0.695 to 0.846 on smaller datasets; (3) Quantization Aware Training reduced inference time by 9.45% while maintaining mAP at 0.819. All tests conducted on Odroid N2+ embedded platform.
                </p>
            </div>
        </div>
        <div class="research-proj block">
            <div class="research-summary">
                <div style="overflow:hidden">
                    <div style="max-width: 50%; float:left;">
                        <span class="pub-name" style="color:black;">People Counting in ROI Area of Multiple CCTVs</span>
                    </div>
                    <div style="max-width: 50%; float:right;">
                        D-soft jsc, 2024
                    </div>
                </div>

                <p class="pub-intro" style="font-size:14px; padding:5px 0px 5px 0px;">
                    Developed a real-time people counting system using YOLOv8n model to detect and count individuals within user-defined ROI areas from multiple CCTV feeds. The system delivers approximately 98% accuracy, 40ms processing time per frame, and around 25 fps performance on NVIDIA GTX 1650 (4GB). Integrated OpenCV and Streamlit for robust data visualization and real-time analytical capabilities.
                </p>
            </div>
        </div>
        <div class="research-proj block">
            <div class="research-summary">
                <div style="overflow:hidden">
                    <div style="max-width: 50%; float:left;">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-74127-2_8" class="pub-name" style="display:inline" target="_blank">Enhanced Attention-based Multimodal Deep Learning for Product Categorization</a>
                    </div>
                    <div style="max-width: 50%; float:right;">
                        DUT, 2023-2024
                    </div>
                </div>

                <p class="pub-intro" style="font-size:14px; padding:5px 0px 5px 0px;">
                    Designed and implemented an attention-based multimodal deep learning model integrating image and text data with a robust fusion module to classify 16 product categories on e-commerce platforms. The model achieved 91.18% accuracy, significantly outperforming traditional multimodal and unimodal models (max 77.21%), enhancing product searchability and customer experience.
                </p>
            </div>
        </div>
        <div class="research-proj block">
            <div class="research-summary">
                <div style="overflow:hidden">
                    <div style="max-width: 50%; float:left;">
                        <span class="pub-name" style="color:black;">NIRsViT: Modified Vision Transformer for Imbalanced NIRS Data Classification</span>
                    </div>
                    <div style="max-width: 50%; float:right;">
                        DUT, 2023
                    </div>
                </div>

                <p class="pub-intro" style="font-size:14px; padding:5px 0px 5px 0px;">
                    Led the design and implementation of NIRsViT, a Vision Transformer-based model tailored for manure classification using near-infrared spectroscopy (NIRS) data. Introduced Focal Loss and Upsampling methods to tackle imbalanced datasets, achieving F1-Score of 93.03% and accuracy of 97.96%, outperforming existing models and establishing a new benchmark in NIRS-based manure identification.
                </p>
            </div>
        </div>
    </div>
    

    <div>

    <div class="block">
        <p class="block" style="float:right">Theme from <a href="https://jonbarron.info/" target="_blank">Jon Barron</a>.</p>
    </div>
    
  </body>
</html>
