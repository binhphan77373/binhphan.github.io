<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="../assets/css/reset.css">
    <link rel="stylesheet" type="text/css" href="../assets/css/stylesheet.css">
    <link rel="stylesheet" type="text/css" href="../assets/css/pubs_webpage.css">

    <script src="../assets/js/simplejs.js"></script>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="../assets/js/youtube.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex: { equationNumbers: { autoNumber: "AMS" } }
      });
      </script>
      
    <link rel="icon" href="../assets/images/projects/duckietown/favicon.ico">

    <title>Controlling car in Duckietown using Deep Q-Learning</title>
  </head>
  
  <body class="body">
        <h1 id="pubs-heading">Controlling car in Duckietown using Deep Q-Learning</h1>
        <h3 id="pubs-author">Lim Sze Chi, Tran Phong</h3>

        <div class="pubs-block">
            <p style="text-align:center; font-size:16px">CS5478 Intelligent Robots: Algorithms and Systems</p>
            <p style="text-align:center; font-size:16px">National University of Singapore, Singapore</p>
        </div>

        <div class="pubs-block">
            <div class="pubs-section pubs-block">Introduction</div>
            <div class="pubs-text pubs-block">
                The purpose of the project is to control a car in a duckietown
                environment to run at the fastest speed and slow down when it is near the traffic light.
                We used an end-to-end neural network learning approach, specifically a Deep
                Q-learning agent (DQN). We trained our DQN agent by experience replay memory and used a Convolutional Neural Network (CNN)
                as a function approximator that maps input observations by Duckiebot to output actions. We used the default rewards as provided
                by the Duckietown repository and adjusted the functions to cope with the traffic light requirements.
            </div>
        </div>

        <div class="divider"></div>
        <div class="pubs-block">
            <div class="pubs-section pubs-block">Methods</div>
            <p class="pubs-block" style="font-size: 18px">1. Architecture</p>
            <div class="pubs-text pubs-block">
                We used DQN for training a policy to output actions given the image inputs. We give the agent high exploration rate 
                at the beginning of training. Hence, the agent takes a random action and stores its
                transition to a memory buffer for multiple episodes most of the time. This memory
                buffer stores all the most recent transitions that an agent took up to a maximum length, and contains (state, action) pairs and their
                corresponding (next_state, reward) result. As the number of episodes increases, the exploration rate decays exponentially as designed
                and the agent will slowly switch to exploitation and execute actions from the CNN model output. Our DQN architecture is shown at Figure 1.
            </div>

            <figure id="pubs-thumbnail">
                <img src="../assets/images/projects/duckietown/dqn.png">
                <figcaption style="font-size:16px" >Figure 1. DQN Architecture.</figcaption>
            </figure>

            <p class="pubs-block" style="font-size: 18px">2. Choice of Input Extractions</p>
            <div class="pubs-text pubs-block">
                We used a CNN architecture from the paper Almasi et. al [1].
                The features extraction steps are similar: resizing input to 12.5% of the original DuckietownEnv observations, 
                and then normalizing pixel values from 0 to 1. We also used their proposed idea of concatenating past observations
                to specially guide the agents at turns. There are some modifications which are furthed detailed in the report.
            </div>

            <p class="pubs-block" style="font-size: 18px">3. Action Representations</p>
            <div class="pubs-text pubs-block">
                The DuckietownEnv accepts the discretized values of velocity in range [0,1] and streering angles spanning from \( [-\pi, +\pi] \).
                In the simplest way, we give a large discretization bins of steering angles but the agent does not respond well to the traffic light 
                because of extremely low speed. We leveraged a pure-pursuit controller [2] to give us the picture of steering angles at the low speed. 
                At low speeds, the pure-pursuit controller outputs a huge range of steering angles from \( 10^{-3} \rightarrow 10^{-1}\). Thus we tried to 
                have a large discretized values around this range and this made the paths smoother when travelling at low speed. 
            </div>

            <p class="pubs-block" style="font-size: 18px">4. Reward Shaping</p>
            <div class="pubs-text pubs-block">
                We use the default rewards provided by the DuckietownEnv. However, because of the requirement of low speed when nearing the traffic
                light, we need to adjust to scale rightly with the default values. The reward shaping and the modification (the orange box) are in Figure 2.
                For a detailed explaination, please refer to the report.
            </div>


            <figure id="pubs-thumbnail">
                <img src="../assets/images/projects/duckietown/reward.png">
                <figcaption style="font-size:16px" >Figure 2. Reward Shaping.</figcaption>
            </figure>

            <p class="pubs-block">[1] Almási, P., Moni, R., & Gyires-Tóth, B. (2020). Robust Reinforcement Learning-based Autonomous Driving Agent for Simulation
                and Real World. Proceedings of the International Joint Conference on Neural Networks.</p>
            <p class="pubs-block">[2] <a href="https://github.com/duckietown/challenge-aido_LF-baseline-behavior-cloning">Pure-pursuit Controller Script</a></p>
        </div>


        <div class="divider"></div>
        <div class="pubs-block">
            <div class="pubs-section pubs-block">Report</div>
            <p class="pubs-block pubs-text">Link to the <a class="pubs-text" href="../assets/pdf/cs5478_report.pdf">report</a></p>
        </div>


        <div class="divider"></div>
        <div class="pubs-block">
            <div class="pubs-section pubs-block">Github Code</div>
            <p class="pubs-block pubs-text">Link to the <a class="pubs-text" href="https://github.com/tpvt99/autonomous-duckie">Github code</a></p>
        </div>

        <div class="divider"></div>
        <div class="pubs-block">
          <div class="pubs-section pubs-block">Discussion</div>
          <p class="pubs-block pubs-text">For any questions regarding the publications, please contact me or post 
            a comment below and I will respond shortly.
          </p>
          <div id="disqus_thread" class="pubs-block"></div>
          <script>
              /**
              *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
              *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
              /*
              var disqus_config = function () {
              this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
              this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
              };
              */
              (function() { // DON'T EDIT BELOW THIS LINE
              var d = document, s = d.createElement('script');
              s.src = 'https://pbtran-github-io.disqus.com/embed.js';
              s.setAttribute('data-timestamp', +new Date());
              (d.head || d.body).appendChild(s);
              })();
          </script>
          <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        </div>

        
  </body>
</html>
